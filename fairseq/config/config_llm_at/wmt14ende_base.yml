data_bin: data-bin/wmt14.en-de_raw
tensorboard_logdir: false

# language config
source_lang: en
target_lang: de

arch: transformer_mod_wmt_en_de
task: translation

ddp_backend: no_c10d
criterion: label_smoothed_cross_entropy
label_smoothing: 0.1
optimizer: adam
lr: 5e-4
lr_scheduler: inverse_sqrt
warmup_init_lr: '1e-07'
stop_min_lr: '1e-09'

# 2 gpus
max_tokens: 16384
update_freq: 2


weight_decay: 0.01

# model hyperparameter
activation_fn: gelu
share_all_embeddings: true

# Train config
seed: 0

log_format: simple
log_interval: 100

warmup_updates: 10000
max_update: 150000

fp16: true
clip_norm: 0.1


keep_best_checkpoints: 5
# best_checkpoint_metric: loss

validate_interval: 500
save_interval: 500
validate_interval_updates: 500
save_interval_updates: 500

keep_interval_updates: 1
keep_last_epochs: 1
keep_best_checkpoints: 5

skip_invalid_size_inputs_valid_test: true

# best_checkpoint_metric: loss
valid_subset: valid
ignore_unused_valid_subsets: true
eval_bleu: true
eval_bleu_print_samples: true
eval_bleu_remove_bpe: true
eval_bleu_detok: space
eval_tokenized_bleu: true
best_checkpoint_metric: bleu
maximize_best_checkpoint_metric: true
eval_bleu_args: "'{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}'"